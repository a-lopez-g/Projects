{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "title : GROVER\n",
    "author : Melissa Sanabria, Jonas Hirsch, Pierre M. Joubert, Anna R. Poetsch\n",
    "date : \"21.08.2024\"\n",
    "output:\n",
    "    html:\n",
    "        code_folding : show\n",
    "        theme : flatly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GROVER](https://www.nature.com/articles/s42256-024-00872-0) (\"Genome Rules Obtained Via Extracted Representations\") is a foundation DNA language model with an optimized vocabulary for the human genome.  \n",
    "This is the Python code to the paper published in Nature Machine Intelligence 2024 https://www.nature.com/articles/s42256-024-00872-0. The R code can be found in a separate document.  \n",
    "\n",
    "Melissa Sanabria, Jonas Hirsch, Pierre M. Joubert, Anna R. Poetsch\n",
    "\n",
    "Biomedical Genomics, Biotechnology Center, Center for Molecular and Cellular Bioengineering, Technische Universit√§t Dresden  \n",
    "melissa.sanabria@tu-dresden.de, arpoetsch@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2\n",
    "\n",
    "Performance based selection of the vocabulary identifies 600 cycles of Byte-Pair Tokenization as optimal\n",
    "\n",
    "### Figure 2A. \n",
    "Selection of the optimal vocabulary through accuracy of next-token prediction as a fine- tuning task for the foundation models using prediction of 2 to 6 nucleotide long next-k-mers as readout. Depicted is accuracy with a loess fit and 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"chr21/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW, BertTokenizer, DataCollatorForLanguageModeling, Trainer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset. 400000 sequences from chromsome 21 for training. And 100000 sequences from chromosome 21 for testing.\n",
    "\n",
    "train_file_path = root_folder + \"train.csv\"\n",
    "test_file_path = root_folder + \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vocabulary information per iteration\n",
    "vocabs_file_path = root_folder + \"vocabs_chr21.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize sequences\n",
    "\n",
    "def find_indices(seq, first, second):\n",
    "    # find indices of all first tokens of most commong bigram\n",
    "    ixs = {}\n",
    "    ix = 0\n",
    "    just_added = False # make sure there are no overlapping tokens like AAA gets tokenized in AA A and not in AA AA\n",
    "    for ch1, ch2 in zip(seq, seq[1:]):\n",
    "        if ((ch1 == first) and (ch2 == second)) and not just_added:\n",
    "            ixs[ix] = 1\n",
    "            just_added = True\n",
    "        else:\n",
    "            just_added = False\n",
    "        ix += 1\n",
    "    return ixs\n",
    "\n",
    "def merge_tokens(seq, ixs, first, second):\n",
    "    # merge most common tokens inplace at the first token (remove second token later)\n",
    "    new_token = first + second\n",
    "    for i in ixs.keys():\n",
    "        seq[i] = new_token\n",
    "    \n",
    "    # remove the token, that got merged with its predecessor token\n",
    "    seq = [x for i, x in enumerate(seq) if i-1 not in ixs]\n",
    "\n",
    "    return seq\n",
    "\n",
    "vocabs = pickle.load(open(vocabs_file_path, \"rb\"))\n",
    "\n",
    "for iteration in vocabs.keys():\n",
    "    iteration_path = root_path + str(iteration)+\"/\"\n",
    "    if not os.path.exists(iteration_path):\n",
    "        os.mkdir(iteration_path)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    data_path = iteration_path + \"data/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    for set_name in [\"train\", \"test\"]:\n",
    "        with open(root_path + set_name + \".txt\", \"r\") as samples_file:\n",
    "            with open(data_path + set_name + \".txt\", \"w\") as iter_set_file:\n",
    "                for sample in samples_file.readlines(): \n",
    "                    tokenized_sequence = list(sample.replace(\"\\n\", \"\"))                        \n",
    "                    for token, info in vocabs[iteration].items(): \n",
    "                        merge_rule = info[1]\n",
    "                        ixs = find_indices(tokenized_sequence, merge_rule[0], merge_rule[1])\n",
    "                        tokenized_sequence = merge_tokens(tokenized_sequence, ixs, merge_rule[0], merge_rule[1])\n",
    "                    sample = \" \".join(tokenized_sequence[:512])\n",
    "                    iter_set_file.write(sample + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is an example for one of the vocabulary iterations. You can repeat that for each one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 600\n",
    "iteration_path = root_path + str(iteration)+\"/\"\n",
    "with open(iteration_path +\"vocab.txt\", \"w\") as vocab_file:\n",
    "        vocab_file.write(\"[PAD]\\n[UNK]\\n[CLS]\\n[SEP]\\n[MASK]\\n\")\n",
    "        for key, value in vocabs[iteration].items():\n",
    "            vocab_file.write(value[0] + \"\\n\")\n",
    "            \n",
    "tokenizer = BertTokenizer.from_pretrained(iteration_path +\"vocab.txt\")\n",
    "\n",
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size=512):\n",
    "\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "\n",
    "            lines = f.read().splitlines()[:-1]\n",
    "            self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
    "\n",
    "\n",
    "test_dataset = LineByLineTextDataset(tokenizer, root_path + str(iteration)+\"/data/test.txt\")\n",
    "train_dataset = LineByLineTextDataset(tokenizer, root_path + str(iteration)+\"/data/train.txt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "        mlm_probability=mlm_probability)\n",
    "\n",
    "\n",
    "model = BertForMaskedLM(config = root_path + \"config.json\")\n",
    "output_path = \"\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    gradient_accumulation_steps=25,\n",
    "    per_gpu_train_batch_size=10,\n",
    "    per_gpu_eval_batch_size=6,\n",
    "    save_steps=500,\n",
    "    save_total_limit=20,\n",
    "    max_steps=20000,\n",
    "    learning_rate=4e-4,\n",
    "    block_size=512,\n",
    "    adam_epsilon=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2 = 0.98,\n",
    "    mlm_probability=0.022,\n",
    "    warmup_steps=1000,\n",
    "    num_train_epochs = max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1,\n",
    "    evaluate_during_training = True,\n",
    "    output_path = output_path\n",
    "    \n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Scripts to reproduce each figure can be found on R code, they contain all the necesary data to reproduce the figures of the paper.\n",
    "\n",
    "However, the following code shows how every data file was generated. You will find a flag **FINAL_FILE** that indicates the generation of a file that will be used in the R code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer.train()\n",
    "trainer.save_model() \n",
    "\n",
    "perplexity_history = {}\n",
    "\n",
    "step = 500\n",
    "for log_history in trainer.state.log_history:\n",
    "    if 'eval_loss' in log_history.keys():\n",
    "        perplexity_history[step] = math.exp(log_history['loss'])\n",
    "        step += 500\n",
    "pickle.dump(perplexity_history, open(root_path + \"perplexity.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning on next-k-mer\n",
    "\n",
    "We choose the training step with best performance (best perplexity) on the test set.\n",
    "\n",
    "Here we will show an example to predict next-2mer. The process for the other next-k-mers is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creation of the next-k-mer dataset\n",
    "next_kmer = 2\n",
    "\n",
    "kmers_vocab = {}\n",
    "kmers_vocab_1 = {\"A\": \"0\", \"C\":\"1\", \"G\":\"2\", \"T\": \"3\"}\n",
    "kmers_vocab = {}\n",
    "count = 0\n",
    "for nuc in kmers_vocabs[1]:\n",
    "    for sec_nuc in kmers_vocabs[1]:\n",
    "        kmers_vocabs[nuc+sec_nuc] = str(count)\n",
    "        count += 1\n",
    "\n",
    "grover_data = pd.read_csv(root_path + str(iteration)+\"/data/test.txt\", header=None, names=[\"sequence\"])\n",
    "grover_data_next_kmer = pd.DataFrame()\n",
    "\n",
    "grover_data_next_kmer[\"sequence\"] = grover_data[\"sequence\"].apply(lambda x: x.split(\" \")[:50])\n",
    "grover_data_next_kmer[\"next_kmer\"] = grover_data[\"sequence\"].apply(lambda x: \"\".join(x.split(\" \")[50:])[:next_kmer])\n",
    "       \n",
    "grover_data_next_kmer[\"class\"] = grover_data_next_kmer[\"next_kmer\"].apply(lambda x: kmer_vocab[x])\n",
    "test_data_grover = pd.DataFrame({'X': grover_data_next_kmer[\"sequence\"], 'class': grover_data_next_kmer[\"class\"]})\n",
    "\n",
    "\n",
    "grover_data = pd.read_csv(root_path + str(iteration)+\"/data/train.txt\", header=None, names=[\"sequence\"])\n",
    "grover_data_next_kmer = pd.DataFrame()\n",
    "\n",
    "grover_data_next_kmer[\"sequence\"] = grover_data[\"sequence\"].apply(lambda x: x.split(\" \")[:50])\n",
    "grover_data_next_kmer[\"next_kmer\"] = grover_data[\"sequence\"].apply(lambda x: \"\".join(x.split(\" \")[50:])[:next_kmer])\n",
    "       \n",
    "grover_data_next_kmer[\"class\"] = grover_data_next_kmer[\"next_kmer\"].apply(lambda x: kmer_vocab[x])\n",
    "train_data_grover = pd.DataFrame({'X': grover_data_next_kmer[\"sequence\"], 'class': grover_data_next_kmer[\"class\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroverDataSet(Dataset):\n",
    "    def __init__(self, sequences, y, tokenizer, max_length=50):\n",
    "        print(\"Loading GROVER Dataset\")\n",
    "        self.BERTtokenizer = tokenizer # to convert ATG ATCGA CG -> [CLS] ATG ATCGA CG [SEP] -> [2, 123, 456, 789, 101, 3]\n",
    "        self.sequences = sequences\n",
    "        self.max_length = max_length\n",
    "        self.y = np.array(y, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "        self.label_encoder = OneHotEncoder(sparse_output=False, categories=[list(range(16))])\n",
    "        self.label_encoder.fit(self.y)\n",
    "        self.y = self.label_encoder.transform(self.y)\n",
    "\n",
    "        self.y = torch.tensor(self.y)\n",
    "        print(self.y.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        tokenizer_res = self.BERTtokenizer.encode_plus(seq, add_special_tokens=True, padding=\"max_length\", return_tensors=\"pt\", max_length=self.max_length, truncation=True)\n",
    "        ids = tokenizer_res[\"input_ids\"].squeeze(0)\n",
    "        attention_masks = tokenizer_res[\"attention_mask\"]\n",
    "        return ids, self.y[idx], attention_masks, idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grover_tokenizer = BertTokenizer.from_pretrained(output_path)\n",
    "train_grover = GroverDataSet(train_data_grover[\"X\"].values, train_data_grover[\"class\"], grover_tokenizer, max_length=50)\n",
    "test_grover = GroverDataSet(test_data_grover[\"X\"].values, test_data_grover[\"class\"], grover_tokenizer, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grover = BertForSequenceClassification.from_pretrained(output_path, num_labels=len(kmer_vocab)).to(device)\n",
    "output_path_next_kmer = \"\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs = 100,\n",
    "    learning_rate = 1e-6,\n",
    "    per_gpu_train_batch_size=16,\n",
    "    per_gpu_eval_batch_size=16,\n",
    "    save_steps = 1000,\n",
    "    logging_steps = 1000,\n",
    "    evaluate_during_training=True,\n",
    "    output_path = output_path_next_kmer\n",
    "    \n",
    ")\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(axis=1)\n",
    "    return {\n",
    "        'accuracy': (preds == labels).mean()\n",
    "    }\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=grover,\n",
    "    train_dataset=train_grover,\n",
    "    eval_dataset=test_grover,\n",
    "    tokenizer=grover_tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "perplexity_history = {}\n",
    "\n",
    "step = 1000\n",
    "for log_history in trainer.state.log_history:\n",
    "    if 'eval_accuracy' in log_history.keys():\n",
    "        perplexity_history[step] = math.exp(log_history['eval_accuracy'])\n",
    "        step += 1000\n",
    "pickle.dump(perplexity_history, open(output_path_next_kmer + \"accuracy.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating performance file\n",
    "results_dict = {}\n",
    "\n",
    "for iteration in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000]:\n",
    "    results_dict[iteration] = {}\n",
    "    for n_mer in range(2, 7):\n",
    "        results_dict[iteration][n_mer] = pickle.load(open(output_path_next_kmer + \"accuracy.pkl\", \"rb\"))\n",
    "        \n",
    "performance_to_save = {}\n",
    "for idx, n_mer in enumerate(range(2, 7)):\n",
    "    performance_to_save[n_mer] = {}\n",
    "    for iteration in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000]:       \n",
    "        performance_to_save[n_mer][iteration] = np.max(results_dict[iteration][n_mer])\n",
    "\n",
    "df = pd.DataFrame.from_dict(performance_to_save)\n",
    "df.to_csv(\"performance.csv\", index=False)\n",
    "## FINAL_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2B. \n",
    "Performance comparison using accuracy of next-k-mer prediction as a fine-tuning task. Compared are GROVER with 600 cycles of Byte-Pair Tokenization with models based on k-mer-tokenization, with length of 4, 5, and 6 nucleotides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['A', 'C', 'G', 'T']\n",
    "for kmer in [4, 5, 6]:\n",
    "    kmer_path = root_path + str(kmer)+\"/\"\n",
    "    if not os.path.exists(kmer_path):\n",
    "        os.mkdir(kmer_path)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    vocab = product(letters, repeat=kmer)\n",
    "    with open(kmer_path +\"vocab.txt\", \"w\") as vocab_file:\n",
    "        vocab_file.write(\"[PAD]\\n[UNK]\\n[CLS]\\n[SEP]\\n[MASK]\\n\")\n",
    "        for token in vocab:\n",
    "            vocab_file.write(token + \"\\n\")\n",
    "    \n",
    "    data_path = kmer_path + \"data/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    for set_name in [\"train\", \"test\"]:\n",
    "        with open(root_path + set_name + \".txt\", \"r\") as samples_file:\n",
    "            with open(data_path + set_name + \".txt\", \"w\") as iter_set_file:\n",
    "                for sample in samples_file.readlines(): \n",
    "                    new_line = \"\"\n",
    "                    count = 0\n",
    "                    for init in range(0, len(sample) - kmer, kmer):\n",
    "                        new_line += sample[init: init + kmer] + \" \"\n",
    "                        count += 1\n",
    "                        if count > 510:\n",
    "                            break\n",
    "                    new_line = \" \".join(new_line[:-1].split(\" \"))\n",
    "                    iter_set_file.write(new_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show how to do it for one k-mer and it can be repeted for the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer = 4\n",
    "\n",
    "kmer_path = root_path + str(kmer)+\"/\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(kmer_path +\"vocab.txt\")\n",
    "\n",
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size=512):\n",
    "\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "\n",
    "            lines = f.read().splitlines()[:-1]\n",
    "            self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
    "\n",
    "\n",
    "test_dataset = LineByLineTextDataset(tokenizer, kmer_path+\"/data/test.txt\")\n",
    "train_dataset = LineByLineTextDataset(tokenizer, kmer_path+\"/data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "        mlm_probability=mlm_probability)\n",
    "\n",
    "\n",
    "model = BertForMaskedLM(config = root_path + \"config_4mer.json\")\n",
    "output_path = \"\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    gradient_accumulation_steps=25,\n",
    "    per_gpu_train_batch_size=10,\n",
    "    per_gpu_eval_batch_size=6,\n",
    "    save_steps=500,\n",
    "    save_total_limit=20,\n",
    "    max_steps=20000,\n",
    "    learning_rate=4e-4,\n",
    "    block_size=512,\n",
    "    adam_epsilon=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2 = 0.98,\n",
    "    mlm_probability=0.022,\n",
    "    warmup_steps=1000,\n",
    "    num_train_epochs = max_steps // (len(train_dataset) // 25) + 1,\n",
    "    evaluate_during_training = True,\n",
    "    output_path = output_path\n",
    "    \n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "perplexity_history = {}\n",
    "\n",
    "step = 500\n",
    "for log_history in trainer.state.log_history:\n",
    "    if 'eval_loss' in log_history.keys():\n",
    "        perplexity_history[step] = math.exp(log_history['loss'])\n",
    "        step += 500\n",
    "pickle.dump(perplexity_history, open(root_path + \"perplexity.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning on next-k-mers\n",
    "\n",
    "We choose the training step with best performance on the test set.\n",
    "\n",
    "Here we will show an example to predict next-2mer. The process for the other next-k-mer is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creation of the next-k-mers dataset\n",
    "next_kmer = 2\n",
    "\n",
    "kmers_vocab = {}\n",
    "kmers_vocab_1 = {\"A\": \"0\", \"C\":\"1\", \"G\":\"2\", \"T\": \"3\"}\n",
    "kmers_vocab = {}\n",
    "count = 0\n",
    "for nuc in kmers_vocabs[1]:\n",
    "    for sec_nuc in kmers_vocabs[1]:\n",
    "        kmers_vocabs[nuc+sec_nuc] = str(count)\n",
    "        count += 1\n",
    "\n",
    "kmer_data = pd.read_csv(kmer_path+\"/data/test.txt\", header=None, names=[\"sequence\"])\n",
    "kmer_data_next_kmer = pd.DataFrame()\n",
    "\n",
    "kmer_data_next_kmer[\"sequence\"] = kmer_data[\"sequence\"].apply(lambda x: x.split(\" \")[:50])\n",
    "kmer_data_next_kmer[\"next_kmer\"] = kmer_data[\"sequence\"].apply(lambda x: \"\".join(x.split(\" \")[50:])[:next_kmer])\n",
    "       \n",
    "kmer_data_next_kmer[\"class\"] = kmer_data_next_kmer[\"next_kmer\"].apply(lambda x: kmer_vocab[x])\n",
    "test_data_kmer = pd.DataFrame({'X': kmer_data_next_kmer[\"sequence\"], 'class': grover_data_next_kmer[\"class\"]})\n",
    "\n",
    "\n",
    "kmer_data = pd.read_csv(kmer_path+\"/data/train.txt\", header=None, names=[\"sequence\"])\n",
    "kmer_data_next_kmer = pd.DataFrame()\n",
    "\n",
    "kmer_data_next_kmer[\"sequence\"] = kmer_data[\"sequence\"].apply(lambda x: x.split(\" \")[:50])\n",
    "kmer_data_next_kmer[\"next_kmer\"] = kmer_data[\"sequence\"].apply(lambda x: \"\".join(x.split(\" \")[50:])[:next_kmer])\n",
    "       \n",
    "kmer_data_next_kmer[\"class\"] = kmer_data_next_kmer[\"next_kmer\"].apply(lambda x: kmer_vocab[x])\n",
    "train_data_kmer = pd.DataFrame({'X': kmer_data_next_kmer[\"sequence\"], 'class': grover_data_next_kmer[\"class\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroverDataSet(Dataset):\n",
    "    def __init__(self, sequences, y, tokenizer, max_length=50):\n",
    "        print(\"Loading GROVER Dataset\")\n",
    "        self.BERTtokenizer = tokenizer # to convert ATG ATCGA CG -> [CLS] ATG ATCGA CG [SEP] -> [2, 123, 456, 789, 101, 3]\n",
    "        self.sequences = sequences\n",
    "        self.max_length = max_length\n",
    "        self.y = np.array(y, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "        self.label_encoder = OneHotEncoder(sparse_output=False, categories=[list(range(16))])\n",
    "        self.label_encoder.fit(self.y)\n",
    "        self.y = self.label_encoder.transform(self.y)\n",
    "\n",
    "        self.y = torch.tensor(self.y)\n",
    "        print(self.y.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        tokenizer_res = self.BERTtokenizer.encode_plus(seq, add_special_tokens=True, padding=\"max_length\", return_tensors=\"pt\", max_length=self.max_length, truncation=True)\n",
    "        ids = tokenizer_res[\"input_ids\"].squeeze(0)\n",
    "        attention_masks = tokenizer_res[\"attention_mask\"]\n",
    "        return ids, self.y[idx], attention_masks, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_tokenizer = BertTokenizer.from_pretrained(output_path)\n",
    "train_kmer = GroverDataSet(train_data_kmer[\"X\"].values, train_data_kmer[\"class\"], kmer_tokenizer, max_length=50)\n",
    "test_kmer = GroverDataSet(test_data_kmer[\"X\"].values, test_data_kmer[\"class\"], kmer_tokenizer, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=len(kmer_vocab)\n",
    "\n",
    "if \"NT\" in model_name:\n",
    "    kmer_model = AutoModelForSequenceClassification.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\", num_labels=num_labels, trust_remote_code=True)\n",
    "elif \"DNABERT2\" in model_name:\n",
    "    kmer_model = AutoModelForSequenceClassification.from_pretrained(\"zhihan1996/DNABERT-2-117M\", num_labels=num_labels, trust_remote_code=True)\n",
    "else:\n",
    "    kmer_model = BertForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
    "\n",
    "kmer_model.to('cuda')\n",
    "\n",
    "if \"NT\" in model_name:\n",
    "    kmer_tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\",trust_remote_code=True)\n",
    "    kmer_tokenizer.eos_token = tokenizer.pad_token\n",
    "elif \"DNABERT2\" in model_name:\n",
    "    kmer_tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "else:\n",
    "    kmer_tokenizer = BertTokenizer.from_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_next_kmer = \"\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                            per_device_train_batch_size=12,\n",
    "                            per_device_eval_batch_size=12,\n",
    "                            evaluation_strategy=\"epoch\", \n",
    "                            num_train_epochs=5,\n",
    "                            logging_strategy=\"epoch\",\n",
    "                            save_strategy=\"epoch\",\n",
    "                            load_best_model_at_end=True,\n",
    "                            learning_rate=5e-6,\n",
    "                            metric_for_best_model=\"f1\",\n",
    "                            output_dir=output_path_next_kmer,\n",
    "                            warmup_steps=100,\n",
    "                            seed=50,\n",
    "                            dataloader_num_workers=10,\n",
    "                            weight_decay=0.01\n",
    "                            )\n",
    "\n",
    "def compute_metrics(logits: np.ndarray, labels: np.ndarray):\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    valid_mask = labels != -100 \n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_labels = labels[valid_mask]\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(valid_labels, valid_predictions),\n",
    "        \"f1\": f1_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"matthews_correlation\": matthews_corrcoef(\n",
    "            valid_labels, valid_predictions\n",
    "        ),\n",
    "        \"precision\": precision_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "    }\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=kmer_model,\n",
    "    train_dataset=train_kmer,\n",
    "    eval_dataset=test_kmer,\n",
    "    tokenizer=kmer_tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_state()\n",
    "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=output_path)\n",
    "\n",
    "results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "with open(os.path.join(output_path_next_kmer, \"eval_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2C. \n",
    "Comparison of accuracy to Term Frequency-Inverse Document Frequency (TF-IDF) models, which use 2 to 6 nucleotide long kmers and the GROVER vocabulary (BPE-600). These models take only token frequencies into account, which are used to train a random forrest model. They are not learning context between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer = 2 # 3,4,5,6\n",
    "sentences = [text for text in train.sequence]\n",
    "X_train = []\n",
    "for text in sentences:\n",
    "    X_train.append(\" \".join([text[i: i + kmer] for i in range(len(text) - kmer + 1)]))\n",
    "\n",
    "sentences = [text for text in test.sequence]\n",
    "X_test = []\n",
    "for text in sentences:\n",
    "    X_test.append(\" \".join([text[i: i + kmer] for i in range(len(text) - kmer + 1)]))\n",
    "\n",
    "sentences = [text for text in val.sequence]\n",
    "X_val = []\n",
    "for text in sentences:\n",
    "    X_val.append(\" \".join([text[i: i + kmer] for i in range(len(text) - kmer + 1)]))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "pickle.dump(vectorizer, open(output_path + \"/TfidfVectorizer.pkl\", \"wb\"))\n",
    "\n",
    "best_val_mcc = -1\n",
    "\n",
    "for n_estimators in range(100, 2001, 200):\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=0, n_jobs=-1)\n",
    "\n",
    "    clf.fit(X_train, train.label)\n",
    "\n",
    "    y_val = clf.predict(X_val)\n",
    "\n",
    "    scores_val = calculate_metric_with_sklearn(val.label, y_val)\n",
    "\n",
    "    if scores_val[\"matthews_correlation\"] > best_val_mcc:\n",
    "        y_test = clf.predict(X_test)\n",
    "\n",
    "        scores_test = calculate_metric_with_sklearn(test.label, y_test)\n",
    "        with open(os.path.join(output_path, \"eval_results.json\"), \"w\") as f:\n",
    "            json.dump(scores_test, f)\n",
    "        \n",
    "        best_val_mcc = scores_test[\"matthews_correlation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\" \".join(tokenizer.tokenize(text)) for text in train.sequence]\n",
    "X_test = [\" \".join(tokenizer.tokenize(text)) for text in test.sequence]\n",
    "X_val = [\" \".join(tokenizer.tokenize(text)) for text in val.sequence]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "pickle.dump(vectorizer, open(output_path + \"/TfidfVectorizer.pkl\", \"wb\"))\n",
    "\n",
    "best_val_mcc = -1\n",
    "\n",
    "for n_estimators in range(100, 2001, 200):\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=0, n_jobs=-1)\n",
    "\n",
    "    clf.fit(X_train, train.label)\n",
    "\n",
    "    y_val = clf.predict(X_val)\n",
    "\n",
    "    scores_val = calculate_metric_with_sklearn(val.label, y_val)\n",
    "\n",
    "    if scores_val[\"matthews_correlation\"] > best_val_mcc:\n",
    "        y_test = clf.predict(X_test)\n",
    "\n",
    "        scores_test = calculate_metric_with_sklearn(test.label, y_test)\n",
    "        with open(os.path.join(output_path, \"eval_results.json\"), \"w\") as f:\n",
    "            json.dump(scores_test, f)\n",
    "        \n",
    "        best_val_mcc = scores_test[\"matthews_correlation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2D. \n",
    "Performance assessment of GROVER with 600 cycles Byte-Pair Tokenization using accuracy for the masked token being predicted as the TOP 1 token, up to TOP 60, i.e. the TOP 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will explain how we obtained the data from the whole reference genome. Then we will show how to train GROVER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"wg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenized chromosomes can be found iin the folder tokenized_chromosomes. In case you want to know they were generated, these are the steps to tokenize the whole genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import argparse\n",
    "from Bio import SeqIO\n",
    "\n",
    "def n_intervals(seq):\n",
    "    for n_code in [\"R\", \"Y\", \"S\", \"W\", \"K\", \"M\", \"B\", \"D\", \"H\", \"V\"]:\n",
    "        seq = seq.replace(n_code, \"N\")\n",
    "\n",
    "    N_positions = [i for i in range(len(seq)) if seq.startswith(\"N\", i)]\n",
    "    N_intervals = []\n",
    "    current_interval = [N_positions[0], 0]\n",
    "    for idx in range(1, len(N_positions)):\n",
    "        if N_positions[idx] != (N_positions[idx - 1] + 1):\n",
    "            current_interval[1] = N_positions[idx - 1] + 1\n",
    "            N_intervals.append([N_positions[idx - 1] + 1, N_positions[idx]])\n",
    "            current_interval[0] = N_positions[idx]\n",
    "    return N_intervals\n",
    "\n",
    "def find_indices(seq, first, second):\n",
    "    # find indices of all first tokens of most commong bigram\n",
    "    ixs = {}\n",
    "    ix = 0\n",
    "    just_added = False # make sure there are no overlapping tokens like AAA gets tokenized in AA A and not in AA AA\n",
    "    for ch1, ch2 in zip(seq, seq[1:]):\n",
    "        if ((ch1 == first) and (ch2 == second)) and not just_added:\n",
    "            ixs[ix] = 1\n",
    "            just_added = True\n",
    "        else:\n",
    "            just_added = False\n",
    "        ix += 1\n",
    "    return ixs\n",
    "\n",
    "def merge_tokens(seq, ixs, first, second):\n",
    "    # merge most common tokens inplace at the first token (remove second token later)\n",
    "    new_token = first + second\n",
    "    for i in ixs.keys():\n",
    "        seq[i] = new_token\n",
    "    \n",
    "    # remove the token, that got merged with its predecessor token\n",
    "    seq = [x for i, x in enumerate(seq) if i-1 not in ixs]\n",
    "\n",
    "    return seq\n",
    "\n",
    "for chrom in range(1, 25):\n",
    "\n",
    "    chrom_file = \"seq_chr_\"+str(chrom)+\".fasta\" ## Fasta file per chromosome\n",
    "    chrom_seq = SeqIO.read(chrom_file, \"fasta\").seq\n",
    "    these_n_intervals = n_intervals(chrom_seq)\n",
    "    sequences = []\n",
    "    for interval in these_n_intervals:\n",
    "        sequences.append(chrom_seq[interval[0] : interval[1]])\n",
    "\n",
    "    vocab_iter_file = root_folder + \"/vocab_info.pkl\"\n",
    "    vocab_iter = pickle.load(open(vocab_iter_file, \"rb\"))\n",
    "\n",
    "    tok_chrom = []\n",
    "    for sequence in sequences:\n",
    "        tokenized_sequence = list(sequence)\n",
    "        for key, value in vocab_iter.items(): \n",
    "            rule = value[1]\n",
    "            ixs = find_indices(tokenized_sequence, rule[0], rule[1])\n",
    "            tokenized_sequence = merge_tokens(tokenized_sequence, ixs, rule[0], rule[1])\n",
    "\n",
    "        tok_chrom.append(tokenized_sequence)\n",
    "        \n",
    "    pickle.dump(tok_chrom, open(\"chr_\"+str(chrom)+\".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the train and test data from the root folder.\n",
    "\n",
    "In case you want to know how we generated the data, this is the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create windows\n",
    "import random\n",
    "import pickle\n",
    "from random import shuffle\n",
    "\n",
    "windows = []\n",
    "for chrom in range(1, 25):\n",
    "    tok_chrom = pickle.load(open(\"tokenized_chromosomes/chr_\"+str(chrom)+\".pkl\", \"rb\"))\n",
    "    for s, sequence in enumerate(tok_chrom):\n",
    "        length_seq = len(sequence)\n",
    "        window_start = 0\n",
    "        remaining_length = length_seq - window_start\n",
    "        while remaining_length > 510:\n",
    "            prob = random.uniform(0, 1)\n",
    "            if prob < 0.5:\n",
    "                length = 510\n",
    "            else:\n",
    "                length = random.randint(20, 510)\n",
    "            windows.append([chrom, s, window_start, window_start + length])\n",
    "            \n",
    "            window_start = window_start + length\n",
    "            remaining_length = length_seq - window_start\n",
    "\n",
    "            \n",
    "shuffle(windows)\n",
    "windows_per_chromosome = {}\n",
    "for w, window in enumerate(windows):\n",
    "    chrom = window[0]\n",
    "    if chrom not in windows_per_chromosome:\n",
    "        windows_per_chromosome[chrom] = {}\n",
    "        windows_per_chromosome[chrom][\"train\"] = []\n",
    "        windows_per_chromosome[chrom][\"test\"] = []\n",
    "    if w > train_length:\n",
    "        split = \"test\"\n",
    "    else:\n",
    "        split = \"train\"\n",
    "    \n",
    "    windows_per_chromosome[chrom][split].append(window[1:])\n",
    "    \n",
    "samples_per_chromosome = {}\n",
    "train_factor = 3\n",
    "test_factor = 2\n",
    "\n",
    "nb_train_windows = 0\n",
    "nb_test_windows = 0\n",
    "for chrom in windows_per_chromosome.keys():\n",
    "    for split in windows_per_chromosome[chrom].keys():\n",
    "        if split in \"train\":\n",
    "            factor = train_factor\n",
    "        if split in \"test\":\n",
    "            factor = test_factor\n",
    "        \n",
    "        new_windows = []\n",
    "        for window in windows_per_chromosome[chrom][split]:\n",
    "            window_start = window[-2]\n",
    "            window_end = window[-1]\n",
    "            window_length = window_end - window_start\n",
    "            if window_length > 50:\n",
    "                for i in range(factor):\n",
    "                    random_start = random.randint(window_start + 1, window_end - 20)\n",
    "                    random_end = random.randint(random_start + 20, window_end)\n",
    "                    new_windows.append([window[0], random_start, random_end])\n",
    "        windows_per_chromosome[chrom][split] += new_windows\n",
    "        \n",
    "with open(root_folder + \"train.txt\", \"w\") as train_file:\n",
    "    for chrom in range(1, 25):\n",
    "        tok_chrom = pickle.load(open(\"chr_\"+str(chrom)+\".pkl\", \"rb\"))\n",
    "        for window in windows_per_chromosome[chrom][\"train\"]:\n",
    "            sample = tok_chrom[window[0]][window[1]: window[2]]\n",
    "            train_file.write(\" \".join(sample) + \"\\n\")\n",
    "            \n",
    "with open(root_folder + \"test.txt\", \"w\") as train_file:\n",
    "    for chrom in range(1, 25):\n",
    "        tok_chrom = pickle.load(open(\"chr_\"+str(chrom)+\".pkl\", \"rb\"))\n",
    "        for window in windows_per_chromosome[chrom][\"test\"]:\n",
    "            sample = tok_chrom[window[0]][window[1]: window[2]]\n",
    "            train_file.write(\" \".join(sample) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training GROVER\n",
    "\n",
    "with open(root_path +\"vocab.txt\", \"w\") as vocab_file:\n",
    "        vocab_file.write(\"[PAD]\\n[UNK]\\n[CLS]\\n[SEP]\\n[MASK]\\n\")\n",
    "        for key, value in vocabs[iteration].items():\n",
    "            vocab_file.write(value[0] + \"\\n\")\n",
    "            \n",
    "tokenizer = BertTokenizer.from_pretrained(root_path +\"vocab.txt\")\n",
    "\n",
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size=512):\n",
    "\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "\n",
    "            lines = f.read().splitlines()[:-1]\n",
    "            self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
    "\n",
    "\n",
    "test_dataset = LineByLineTextDataset(tokenizer, root_path + \"/test.txt\")\n",
    "train_dataset = LineByLineTextDataset(tokenizer, root_path + \"/train.txt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "        mlm_probability=mlm_probability)\n",
    "\n",
    "\n",
    "model = BertForMaskedLM(config = root_path + \"config.json\")\n",
    "output_path = \"\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    gradient_accumulation_steps=25,\n",
    "    per_gpu_train_batch_size=10,\n",
    "    per_gpu_eval_batch_size=6,\n",
    "    save_steps=500,\n",
    "    save_total_limit=20,\n",
    "    max_steps=20000,\n",
    "    learning_rate=4e-4,\n",
    "    block_size=512,\n",
    "    adam_epsilon=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2 = 0.98,\n",
    "    mlm_probability=0.022,\n",
    "    warmup_steps=1000,\n",
    "    num_train_epochs = max_steps // (len(train_dataset) // 25) + 1,\n",
    "    evaluate_during_training = True,\n",
    "    output_path = output_path\n",
    "    \n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()  \n",
    "\n",
    "perplexity_history = {}\n",
    "\n",
    "step = 500\n",
    "for log_history in trainer.state.log_history:\n",
    "    if 'eval_loss' in log_history.keys():\n",
    "        perplexity_history[step] = math.exp(log_history['loss'])\n",
    "        step += 500\n",
    "pickle.dump(perplexity_history, open(root_path + \"perplexity.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction of a masked token per sample\n",
    "\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "def collate(examples: List[torch.Tensor]):\n",
    "    if tokenizer._pad_token is None:\n",
    "        return pad_sequence(examples, batch_first=True)\n",
    "    return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "eval_sampler = SequentialSampler(test_dataset)\n",
    "eval_dataloader = DataLoader(\n",
    "    test_dataset, sampler=eval_sampler, batch_size=32, collate_fn=collate)\n",
    "\n",
    "\n",
    "model = BertModel.from_pretrained(output_path, config=root_path + \"config.json\")\n",
    "predictions = []\n",
    "labels = []\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        probability_matrix = torch.rand(batch.shape)\n",
    "        special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in batch.tolist()]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if tokenizer._pad_token is not None:\n",
    "            padding_mask = batch.eq(tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "        \n",
    "        token_per_sample = torch.argmax(probability_matrix, dim=1)\n",
    "        \n",
    "        batch[torch.arange(batch.shape[0]), token_per_sample] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "        \n",
    "        batch = batch.to(\"cuda:0\")\n",
    "        outputs = model(batch)\n",
    "        prediction = outputs[0].detach().cpu().numpy()[torch.arange(batch.shape[0]), token_per_sample]\n",
    "        predictions.append(softmax(prediction, axis=-1))\n",
    "        \n",
    "        labels.append(batch[np.arange(batch.shape[0]), token_per_sample.numpy()].numpy())\n",
    "        \n",
    "labels = np.hstack(labels)\n",
    "predictions = np.vstack(predictions)\n",
    "top_k = np.zeros(300)\n",
    "for k in range(1,300):\n",
    "   \n",
    "    top_k[k-1] = top_k_accuracy_score(labels, predictions, k=k, labels=np.arange(609))\n",
    "\n",
    "    print(k, top_k[k-1] )\n",
    "\n",
    "pickle.dump(top_k, open(root_path + \"top_k.pkl\", \"wb\")) ## FINAL_FILE\n",
    "pickle.dump(labels, open(root_path + \"labels.pkl\", \"wb\"))\n",
    "pickle.dump(predictions, open(root_path + \"predictions.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2E.  \n",
    "Performance assessment of GROVER with 600 cycles Byte-Pair Tokenization using perplexity, divided by the total number of words in the dictionary. Comparison with models based on k-mer-tokenization, with length of 4, 5, and 6 nucleotides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3\n",
    "The frequency balanced GROVER vocabulary shows differential learning performance by token length\n",
    "\n",
    "To generate Figure 3F. and Figure 3G. we need the metrics per token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"weighted\"):\n",
    "    \n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = list(set(actual_class))\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        \n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "        \n",
    "    return roc_auc_dict\n",
    "\n",
    "\n",
    "labels = pickle.load(open(root_path + \"labels.pkl\", \"rb\"))\n",
    "predictions = pickle.load(open(root_path + \"predictions.pkl\", \"rb\"))\n",
    "\n",
    "acc_per_token = np.zeros((609, 300))\n",
    "for k in range(300):\n",
    "    for t in range(609):\n",
    "        this_token_ids = np.where(labels == t)[0]\n",
    "        if len(this_token_ids) > 0:\n",
    "            this_token_labels = labels[this_token_ids]\n",
    "            this_token_predictions = predictions[this_token_ids]\n",
    "            acc_per_token[t, k] = top_k_accuracy_score(this_token_labels, this_token_predictions, k=k, labels=np.arange(609))\n",
    "        \n",
    "\n",
    "auc_per_token = roc_auc_score_multiclass(labels, predictions, \"macro\")        \n",
    "        \n",
    "        \n",
    "output_file_path = \"metrics_per_token.csv\" ## FINAL_FILE\n",
    "with open(output_file_path, \"w\") as output:\n",
    "    header = \"token_id,token,auc,\"\n",
    "    for k in range(1,301):\n",
    "        header += \"top\"+str(k)+\",\"\n",
    "    header = header[:-1] ## Remove comma\n",
    "    output.write(header + \"\\n\")\n",
    "    for t, token in enumerate(tokens):\n",
    "        auc = 0\n",
    "        if t in auc_per_token:\n",
    "            auc = auc_per_token[t]\n",
    "        \n",
    "        line_to_write = str(t) +\",\"+ token +\",\"+ str(auc) +\",\"\n",
    "        for k in range(300):\n",
    "            line_to_write += str(acc_per_token[t,k]) + \",\"\n",
    "        output.write(line_to_write[:-1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 4\n",
    "Average GROVER token embedding shows learning of genome information content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "with open(root_path + \"/train.txt\") as train_file:\n",
    "    data = train_file.readlines()[:-1]\n",
    "sent = [row.split() for row in data[:300000]]\n",
    "w2v_model = gensim.models.Word2Vec(sent, min_count = 1, vector_size = 768, window = 5, workers=10)\n",
    "\n",
    "pickle.dump(w2v_model.wv.vectors, open(root_path + \"iter600_w2v.pkl\", \"wb\")) ## FINAL_FILE\n",
    "\n",
    "with open(root_path + \"iter600_w2v_vocab.txt\", \"w\") as vocab_file: ## FINAL_FILE\n",
    "    for word in w2v_model.wv.index_to_key:\n",
    "        vocab_file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(output_path, config=root_path + \"config.json\")\n",
    "\n",
    "vocabulary = \"\"\n",
    "with open(root_path +\"vocab.txt\", \"r\") as vocabulary_file:\n",
    "    for line in vocabulary_file.readlines()[5:]:\n",
    "        vocabulary += line.replace(\"\\n\", \"\") + \" \"\n",
    "        \n",
    "embedding_matrix = model.embeddings.word_embeddings.weight.to(\"cpu\").detach().numpy()\n",
    "\n",
    "pickle.dump(embedding_matrix, open(\"vocab_embedding.pkl\", \"wb\")) ## FINAL_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5\n",
    "GROVER learns token context and genome annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5A.\n",
    "Self-similarity per token sequence as extracted by cosine similarity of the same token in different contexts throughout the 12 transformer layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(examples: List[torch.Tensor]):\n",
    "    if tokenizer._pad_token is None:\n",
    "        return pad_sequence(examples, batch_first=True)\n",
    "    return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "eval_sampler = SequentialSampler(test_dataset)\n",
    "eval_dataloader = DataLoader(\n",
    "    test_dataset, sampler=eval_sampler, batch_size=32, collate_fn=collate)\n",
    "\n",
    "model = BertModel.from_pretrained(output_path, config=root_path + \"config.json\")\n",
    "model.eval()\n",
    "count = 0\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    with torch.no_grad():\n",
    "        batch = batch.to(\"cuda:0\")\n",
    "        outputs = model(batch)\n",
    "        embeddings = torch.stack(list(outputs[1]), dim=0).detach().cpu().numpy()\n",
    "        pickle.dump(embeddings, open(\"embeddings_test/\"+str(count)+\".pkl\", \"wb\"))\n",
    "        del embeddings\n",
    "        count += 1\n",
    "\n",
    "\n",
    "## Group the embeddings per token\n",
    "tokens = []\n",
    "with open(root_path +\"vocab.txt\") as vocab_file:\n",
    "    for line in vocab_file.readlines():\n",
    "        tokens.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "tokens_to_get = np.asarray([[7] + list(range(9, len(tokens)))][0])\n",
    "\n",
    "total_samples = 5000\n",
    "\n",
    "for layer in range(1,13):\n",
    "    token_embeddings = [[] for _ in range(len(tokens_to_get))]\n",
    "    token_counts = [0 for _ in range(len(tokens_to_get))]\n",
    "    count = 0\n",
    "    for batch in eval_dataloader:\n",
    "        batch = batch.numpy()\n",
    "        if not os.path.exists(\"embeddings_test/\"+str(count)+\".pkl\"):\n",
    "            break\n",
    "        embeddings = pickle.load(open(\"embeddings_test/\"+str(count)+\".pkl\", \"rb\"))[layer]\n",
    "        for t_idx, token_id in enumerate(tokens_to_get):\n",
    "            if token_counts[t_idx] >= total_samples:\n",
    "                print(\"Token \",tokens[token_id], \"is full\")\n",
    "                continue   \n",
    "            condition = np.nonzero(batch == token_id)\n",
    "            times = len(condition[0])\n",
    "            if times > 0:   \n",
    "                token_embeddings[t_idx].append(embeddings[condition])\n",
    "                token_counts[t_idx] += times\n",
    "\n",
    "\n",
    "        del embeddings\n",
    "        count += 1\n",
    "        all_full = True\n",
    "        for t_idx in range(len(tokens_to_get)):\n",
    "            if token_counts[t_idx] < total_samples:\n",
    "                all_full = False\n",
    "                break\n",
    "        if all_full:\n",
    "            break\n",
    "    for t_idx, token_id in enumerate(tokens_to_get):\n",
    "        print(\"Saving file for token \", tokens[token_id])\n",
    "        pickle.dump(np.vstack(token_embeddings[t_idx]), open(str(token_id)+\"_layer_\"+str(layer)+\".pkl\", \"wb\"))\n",
    "        \n",
    "self_sim_per_token = np.zeros((len(tokens_to_get), 12))\n",
    "for t_idx, token_id in enumerate(tokens_to_get):\n",
    "    for l_idx, layer in enumerate(range(1,13)):\n",
    "        token_embeddings = pickle.load(open(str(token_id)+\"_layer_\"+str(layer)+\".pkl\", \"rb\"))[:5000]\n",
    "        sim = cosine_similarity(token_embeddings)\n",
    "        upper = sim[np.triu_indices(len(token_embeddings), k = 1)]\n",
    "        self_sim_per_token[t_idx, l_idx] = upper.mean()\n",
    "        \n",
    "        \n",
    "output_file_path = \"self_similarity_per_token.csv\" ## FINAL_FILE\n",
    "with open(output_file_path, \"w\") as output:\n",
    "    header = \"token_id,token,\"\n",
    "    for k in range(12):\n",
    "        header += \"layer\"+str(k + 1)+\",\"\n",
    "    header = header[:-1] \n",
    "    output.write(header + \"\\n\")\n",
    "    for t_idx, t in enumerate(tokens_to_get):\n",
    "        token = tokens[t]\n",
    "        line_to_write = str(t) +\",\"+ token +\",\"\n",
    "        for k in range(12):\n",
    "            line_to_write += str(self_sim_per_token[t_idx,k]) + \",\"\n",
    "        output.write(line_to_write[:-1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5B,5C,5D.\n",
    "Embedding of regions in the genome 510 tokens in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from transformers import BertTokenizer,  BertModel, PreTrainedTokenizer\n",
    "from torch.utils.data import DataLoader, SequentialSampler, Dataset\n",
    "\n",
    "\n",
    "class notNsample(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, tokenized_chromosome, indices_list_path, sample_size, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.tokenized_chromosome = tokenized_chromosome\n",
    "        self.indices_list = []\n",
    "\n",
    "        if os.path.exists(indices_list_path):\n",
    "            print(\"Loading indices_list from cached file %s\", indices_list_path)\n",
    "            with open(indices_list_path, \"rb\") as handle:\n",
    "                self.indices_list = pickle.load(handle)\n",
    "        else:\n",
    "            print(\"Creating features from dataset file at\", indices_list_path)\n",
    "            for start in range(0, len(tokenized_chromosome) - sample_size, 510):\n",
    "                this_sample = tokenized_chromosome[start: start + sample_size]\n",
    "                if \"N\" not in this_sample:\n",
    "                    self.indices_list.append([start, start + sample_size])\n",
    "            pickle.dump(self.indices_list, open(indices_list_path, \"wb\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = self.indices_list[i][0]\n",
    "        end = self.indices_list[i][1]\n",
    "        sample = self.tokenized_chromosome[start:end]\n",
    "        tokenized_text = self.tokenizer.convert_tokens_to_ids(sample)\n",
    "        tokenized_text = self.tokenizer.build_inputs_with_special_tokens(tokenized_text)\n",
    "        return torch.tensor(tokenized_text, dtype=torch.long)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--chromosome\", default=None, type=str, required=True\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    chromosome = args.chromosome\n",
    "    output_path = \"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    ## Create data\n",
    "    tokenized_chromosome = pickle.load(open(\"tokenized_chromosomes/chr_\"+chromosome+\".pkl\", \"rb\"))\n",
    "    sample_size = 510\n",
    "    batch_size = 32\n",
    "\n",
    "    model_path = \"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertModel.from_pretrained(model_path)\n",
    "    model.to('cuda')\n",
    "\n",
    "    def collate(examples):\n",
    "        samples = pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        \n",
    "        attention_mask = torch.ones_like(samples)\n",
    "        attention_mask[samples == tokenizer.pad_token_id] = 0\n",
    "\n",
    "        return {\"samples\": samples, \"attention_mask\": attention_mask}\n",
    "\n",
    "    indices_list_path = output_path + \"samples_chr_\"+chromosome+\"_indices_list.pkl\"\n",
    "    eval_dataset = notNsample(tokenizer, tokenized_chromosome, indices_list_path, sample_size)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=batch_size, collate_fn=collate\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    ## Predicting\n",
    "    \n",
    "    embeddings = []\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        with torch.no_grad():\n",
    "            samples = batch[\"samples\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            batch = samples.to(\"cuda\")\n",
    "            attention_mask = attention_mask.to(\"cuda:0\")\n",
    "            outputs = model(batch, attention_mask = attention_mask)\n",
    "            outputs = model(batch)[0][:,0]\n",
    "            embeddings += outputs.detach().cpu().tolist()\n",
    "\n",
    "    pickle.dump(embeddings, open(output_path + \"chr_\"+chromosome+\"_cls_tokens.pkl\", \"wb\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 6.\n",
    "GROVER outperforms other models for biological fine-tuning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Figure 6B,6C,6E,6F,6H,6I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets\n",
    "\n",
    "annotations come from: https://epd.epfl.ch/human/human_database.php?db=human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"finetuning_tasks/Prom300/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(root_path+'promoterRangesHg19.bed', sep='\\t', header=None)\n",
    "annotations.columns = ['chr', 'start', 'end', 'name', 'score', 'strand']\n",
    "annotations.sort_values(by=['chr', 'start'], inplace=True)\n",
    "\n",
    "annotations[\"start\"] = annotations.apply(lambda x: x[\"start\"]-249 if x[\"strand\"] == \"+\" else x[\"start\"]-50, axis=1)\n",
    "annotations[\"end\"] = annotations.apply(lambda x: x[\"end\"]+50 if x[\"strand\"] == \"+\" else x[\"end\"]+249, axis=1)\n",
    "\n",
    "\n",
    "# load BP chromosome mapper\n",
    "def load_mapper(BP_chr):\n",
    "    # create mapper that maps nucleotide position to BP position\n",
    "    mapper = []\n",
    "    curr_BP_pos = 0\n",
    "    for i in range(len(BP_chr)):\n",
    "        for e in range(len(BP_chr[i])):\n",
    "            mapper.append(i)\n",
    "    return mapper\n",
    "\n",
    "\n",
    "# iterate through each chromosome\n",
    "BP_seqs = []\n",
    "for chrom in annotations[\"chr\"].unique():\n",
    "    chrom_nr = chrom[3:]\n",
    "    if chrom_nr == \"X\":\n",
    "        chrom_nr = 23\n",
    "    elif chrom_nr == \"Y\":\n",
    "        chrom_nr = 24\n",
    "    # slice the annotations for the current chromosome\n",
    "    annotations_chr = annotations.loc[annotations[\"chr\"] == chrom]\n",
    "    # load BP tokenized chromosome\n",
    "    with open(f\"tokenized_chromosomes/chr_{chrom_nr}.pkl\", \"rb\") as f:\n",
    "        BP_chr = pickle.load(f)\n",
    "    \n",
    "    # load mapper that maps nucleotide position to BP position\n",
    "    print(f\"loading mapper {chrom}\")\n",
    "    mapper = load_mapper(BP_chr)\n",
    "\n",
    "    BP_starts = annotations.loc[annotations[\"chr\"] == chrom][\"start\"].apply(lambda x: mapper[x])\n",
    "    BP_ends = annotations.loc[annotations[\"chr\"] == chrom][\"end\"].apply(lambda x: mapper[x])\n",
    "    for start, end in zip(BP_starts, BP_ends):\n",
    "        BP_seqs.append(BP_chr[start:end])\n",
    "\n",
    "annotations[\"BPseq\"] = BP_seqs\n",
    "annotations[\"sequence\"] = annotations[\"BPseq\"].apply(lambda x: list(\"\".join(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random mutate\n",
    "\n",
    "Preprocessing like https://github.com/egochao/DeePromoter\n",
    "\n",
    "Procedure for create negative dataset as described in paper:\n",
    "\n",
    "    Step 1: Break the sequence in N parts(20 as in the paper)\n",
    "\n",
    "    Old Step 2: Random choose M parts of the original sequence to keep it, and random initialize the rest\n",
    "\n",
    "    New Step 2: Randomly choose M parts of the original sequence to keep it, shuffle the rest around (preserves sequence attributes like GC content and AG balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the indices of all elements of k chunks of size (len(lst) // k) in a list\n",
    "def get_chunks(lst, k):\n",
    "    chunkSize = len(lst) // k\n",
    "    indices = []\n",
    "    for i in range(0, len(lst), chunkSize):\n",
    "        indices.append([e for e in range(i, min(i + chunkSize, len(lst)))]) # min is used to avoid index out of bounds\n",
    "    return indices\n",
    "\n",
    "def choose_random_chunks(chunks, nrOfChunksToMutate):\n",
    "    random_indices = np.random.choice(len(chunks), nrOfChunksToMutate, replace=False)\n",
    "    return [chunks[i] for i in random_indices]\n",
    "\n",
    "def unroll_chunks(chunks):\n",
    "    return [e for chunk in chunks for e in chunk]\n",
    "\n",
    "def shuffleIndices(indices):\n",
    "    shuffleIndices = indices.copy()\n",
    "    np.random.shuffle(shuffleIndices)\n",
    "    return shuffleIndices\n",
    "\n",
    "def random_mutate(seq, k, nrOfChunksToMutate):\n",
    "    seq = seq.copy()\n",
    "    chunks = get_chunks(seq, k=k)\n",
    "    chunksToMutate = choose_random_chunks(chunks, nrOfChunksToMutate=nrOfChunksToMutate)\n",
    "    indicesToMutate = unroll_chunks(chunksToMutate)\n",
    "    shuffledIndicesToMutate = shuffleIndices(indicesToMutate)\n",
    "    for x,y in zip(indicesToMutate,shuffledIndicesToMutate):\n",
    "        seq[x] = seq[y]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffle GROVER sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = annotations.loc[1, 'BPseq']\n",
    "nrOfChunks = 8 # in how many splits do we want to divide the sequence\n",
    "nrOfChunksToMutate = 6 # how many of these splits do we want to mutate (randomly shuffle)\n",
    "\n",
    "annotations[\"BPseqMutated\"] = annotations[\"BPseq\"].apply(lambda x: random_mutate(x, nrOfChunks, nrOfChunksToMutate))\n",
    "\n",
    "data_non_mutated = pd.DataFrame({\"X\": annotations[\"BPseq\"], \"y\": [1] * len(annotations), \"start\": annotations[\"start\"], \"end\": annotations[\"end\"], \"name\": annotations[\"name\"], \"strand\": annotations[\"strand\"], \"chr\": annotations[\"chr\"]})\n",
    "data_mutated = pd.DataFrame({\"X\": annotations[\"BPseqMutated\"], \"y\": [0] * len(annotations), \"start\": annotations[\"start\"], \"end\": annotations[\"end\"], \"name\": annotations[\"name\"], \"strand\": annotations[\"strand\"], \"chr\": annotations[\"chr\"]})\n",
    "data = pd.concat([data_non_mutated, data_mutated], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, val, test: 80%, 10%, 10%\n",
    "train = data.sample(frac=0.8, random_state=42)\n",
    "val_test = data.drop(train.index)\n",
    "val = val_test.sample(frac=0.5, random_state=42)\n",
    "test = val_test.drop(val.index)\n",
    "\n",
    "train.to_csv(root_path + \"train.tsv\", index=False, sep='\\t')\n",
    "val.to_csv(root_path + \"validate.tsv\", index=False, sep='\\t')\n",
    "test.to_csv(root_path + \"test.tsv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"finetuning_tasks/PromScan/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(root_path + 'promoterRangesHg19.bed', sep='\\t', header=None)\n",
    "annotations.columns = ['chr', 'start', 'end', 'name', 'score', 'strand']\n",
    "\n",
    "annotations[\"tss\"] = annotations[\"start\"].copy()\n",
    "annotations[\"start\"] = annotations[\"start\"] - 5000\n",
    "annotations[\"end\"] = annotations[\"end\"] + 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split long sequences in overlapping windows of size 1001 nucleotides and stride of 300 nucl.\n",
    "\n",
    "def split_in_overlapping_windows(start, end, window_size, stride, tss_pos, chr, strand, name):\n",
    "    windows = []\n",
    "    for i in range(start, end, stride):\n",
    "        this_start = i\n",
    "        this_end = i + window_size\n",
    "        this_window = {\"start\": this_start, \"end\": this_end, \"chr\": chr, \"is_ tss\": this_start <= tss_pos and tss_pos <= this_end, \"strand\": strand, \"name\": name}\n",
    "        windows.append(this_window)\n",
    "    return windows\n",
    "\n",
    "stacked_windows = annotations.apply(lambda x: split_in_overlapping_windows(x[\"start\"], x[\"end\"], 1001, 300, x[\"tss\"], x[\"chr\"], x[\"strand\"], x[\"name\"]), axis=1)\n",
    "\n",
    "\n",
    "windows_dict = {\"start\": [], \"end\": [], \"chr\": [], \"is_tss\": [], \"strand\": [], \"name\": []}\n",
    "\n",
    "for sample in stacked_windows:\n",
    "    for window in sample:\n",
    "        windows_dict[\"start\"].append(window[\"start\"])\n",
    "        windows_dict[\"end\"].append(window[\"end\"])\n",
    "        windows_dict[\"chr\"].append(window[\"chr\"])\n",
    "        windows_dict[\"is_tss\"].append(window[\"is_ tss\"])\n",
    "        windows_dict[\"strand\"].append(window[\"strand\"])\n",
    "        windows_dict[\"name\"].append(window[\"name\"])\n",
    "        \n",
    "window_df = pd.DataFrame(windows_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BP chromosome mapper\n",
    "def load_mapper(BP_chr):\n",
    "    # create mapper that maps nucleotide position to BP position\n",
    "    mapper = []\n",
    "    curr_BP_pos = 0\n",
    "    for i in range(len(BP_chr)):\n",
    "        for e in range(len(BP_chr[i])):\n",
    "            mapper.append(i)\n",
    "    return mapper\n",
    "\n",
    "\n",
    "# iterate through each chromosome\n",
    "BP_seqs = []\n",
    "for chrom in annotations[\"chr\"].unique():\n",
    "    chrom_nr = chrom[3:]\n",
    "    if chrom_nr == \"X\":\n",
    "        chrom_nr = 23\n",
    "    elif chrom_nr == \"Y\":\n",
    "        chrom_nr = 24\n",
    "    # slice the annotations for the current chromosome\n",
    "    annotations_chr = window_df.loc[window_df[\"chr\"] == chrom]\n",
    "    # load BP tokenized chromosome\n",
    "    with open(f\"tokenized_chromosomes/chr_{chrom_nr}.pkl\", \"rb\") as f:\n",
    "        BP_chr = pickle.load(f)\n",
    "    \n",
    "    # load mapper that maps nucleotide position to BP position\n",
    "    print(f\"loading mapper {chrom}\")\n",
    "    mapper = load_mapper(BP_chr)\n",
    "\n",
    "    BP_starts = window_df.loc[window_df[\"chr\"] == chrom][\"start\"].apply(lambda x: mapper[x])\n",
    "    BP_ends = window_df.loc[window_df[\"chr\"] == chrom][\"end\"].apply(lambda x: mapper[x])\n",
    "    for start, end in zip(BP_starts, BP_ends):\n",
    "        BP_seqs.append(BP_chr[start:end])\n",
    "\n",
    "window_df[\"BPseq\"] = BP_seqs\n",
    "window_df[\"sequence\"] = annotations[\"BPseq\"].apply(lambda x: list(\"\".join(x)))\n",
    "\n",
    "window_df.rename(columns={\"BPseq\": \"X\", \"is_tss\": \"y\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, val, test: 80/10/10\n",
    "train, validate, test = np.split(window_df.sample(frac=1, random_state=42), [int(.8*len(window_df)), int(.9*len(window_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(root_path + \"train.tsv\", index=False, sep=\"\\t\")\n",
    "validate.to_csv(root_path + \"validate.tsv\", index=False, sep=\"\\t\")\n",
    "test.to_csv(root_path + \"test.tsv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTCF peaks from https://www.encodeproject.org/experiments/ENCSR000BIE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"finetuning_tasks/TF_binding/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks = pd.read_csv(root_path + 'ENCFF915BIE.bed', sep='\\t', header=None)\n",
    "motif_sites  = pd.read_csv(root_path + 'CTCF_motif_sites.gff', sep='\\t', header=None, skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run in command line:\n",
    "- intersectBed -loj -a CTCF_motif_sites.gff -b ENCFF915BIE.bed > CTCF_motifs_with_peak_annotation.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_with_peaks = pd.read_csv(root_path + 'CTCF_motifs_with_peak_annotation.bed', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the center of motif\n",
    "motifs_with_peaks[\"center_of_motif\"] = motifs_with_peaks[3] + (motifs_with_peaks[4] - motifs_with_peaks[3])//2\n",
    "\n",
    "## Get 1kb area around motif\n",
    "motifs_with_peaks[\"start_of_bin\"] = motifs_with_peaks[\"center_of_motif\"] - 500\n",
    "motifs_with_peaks[\"end_of_bin\"] = motifs_with_peaks[\"center_of_motif\"] + 500\n",
    "motifs_with_peaks[\"width\"] = motifs_with_peaks[\"end_of_bin\"] - motifs_with_peaks[\"start_of_bin\"]\n",
    "\n",
    "## Add target annotation column for machine learning task\n",
    "motifs_with_peaks[\"y\"] = motifs_with_peaks[9].apply(lambda x: 1 if x != '.' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve GROVER tokens\n",
    "\n",
    "data = motifs_with_peaks[[0, \"start_of_bin\", \"end_of_bin\", \"y\"]]\n",
    "data = data.loc[data[0].apply(lambda x: True if len(x) <= 5 else False)] # some random lines with chr1_gl000191_random etc\n",
    "data = data.sort_values(by=[0, \"start_of_bin\"], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapper(BP_chr):\n",
    "    # create mapper that maps nucleotide position to BP position\n",
    "    mapper = []\n",
    "    for i in range(len(BP_chr)):\n",
    "        for e in range(len(BP_chr[i])):\n",
    "            mapper.append(i)\n",
    "    return mapper\n",
    "\n",
    "# iterate through each chromosome\n",
    "BP_seqs = []\n",
    "for chrom in data[0].unique():\n",
    "    chrom_nr = chrom[3:]\n",
    "    if chrom_nr == \"X\":\n",
    "        chrom_nr = 23\n",
    "    elif chrom_nr == \"Y\":\n",
    "        chrom_nr = 24\n",
    "    # slice the data for the current chromosome\n",
    "    data_chr = data.loc[data[0] == chrom]\n",
    "    # load BP tokenized chromosome\n",
    "    with open(f\"tokenized_chromosomes/chr_{chrom_nr}.pkl\", \"rb\") as f:\n",
    "        BP_chr = pickle.load(f)\n",
    "    \n",
    "    # load mapper that maps nucleotide position to BP position\n",
    "    print(f\"loading mapper {chrom}\")\n",
    "    mapper = load_mapper(BP_chr)\n",
    "\n",
    "    BP_starts = data.loc[data[0] == chrom][\"start_of_bin\"].apply(lambda x: mapper[x])\n",
    "    BP_ends = data.loc[data[0] == chrom][\"end_of_bin\"].apply(lambda x: mapper[x])\n",
    "    for start, end in zip(BP_starts, BP_ends):\n",
    "        BP_seqs.append(BP_chr[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"X\"] = BP_seqs\n",
    "\n",
    "## create train val test (80/10/10)\n",
    "train = data.sample(frac=0.8, random_state=0)\n",
    "val = data.drop(train.index)\n",
    "test = val.sample(frac=0.5, random_state=0)\n",
    "val = val.drop(test.index)\n",
    "\n",
    "train.to_csv(root_path + \"CTCF_train.tsv\", index=False, sep=\"\\t\")\n",
    "val.to_csv(root_path + \"CTCF_val.tsv\", index=False, sep=\"\\t\")\n",
    "test.to_csv(root_path + \"CTCF_test.tsv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Sequence\n",
    "from sklearn.metrics import matthews_corrcoef, precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, BertTokenizer, PreTrainedTokenizerFast, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    run_name: str = field(default=\"run\")\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(default=512, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    gradient_accumulation_steps: int = field(default=1)\n",
    "    per_device_train_batch_size: int = field(default=1)\n",
    "    per_device_eval_batch_size: int = field(default=1)\n",
    "    num_train_epochs: int = field(default=1)\n",
    "    fp16: bool = field(default=False)\n",
    "    logging_steps: int = field(default=100)\n",
    "    save_steps: int = field(default=100)\n",
    "    eval_steps: int = field(default=100)\n",
    "    evaluation_strategy: str = field(default=\"steps\"),\n",
    "    warmup_steps: int = field(default=50)\n",
    "    weight_decay: float = field(default=0.01)\n",
    "    learning_rate: float = field(default=1e-4)\n",
    "    save_total_limit: int = field(default=3)\n",
    "    load_best_model_at_end: bool = field(default=True)\n",
    "    output_dir: str = field(default=\"output\")\n",
    "    find_unused_parameters: bool = field(default=False)\n",
    "    checkpointing: bool = field(default=False)\n",
    "    dataloader_pin_memory: bool = field(default=False)\n",
    "    eval_and_save_results: bool = field(default=True)\n",
    "    save_model: bool = field(default=False)\n",
    "    seed: int = field(default=42)\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length, kmer):\n",
    "\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        texts = [\"\".join(text) for text in texts]\n",
    "\n",
    "        if kmer:\n",
    "            sequences = []\n",
    "            for text in texts:\n",
    "                sequences.append(\" \".join([text[i: i + kmer] for i in range(len(text) - kmer + 1)]))\n",
    "        else:\n",
    "            sequences = texts\n",
    "            \n",
    "        output = tokenizer(\n",
    "            sequences,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        self.input_ids = output[\"input_ids\"]\n",
    "        self.attention_mask = output[\"attention_mask\"]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "    \n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict) \n",
    "\n",
    "def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    valid_mask = labels != -100  \n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_labels = labels[valid_mask]\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(valid_labels, valid_predictions),\n",
    "        \"f1\": f1_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"matthews_correlation\": matthews_corrcoef(\n",
    "            valid_labels, valid_predictions\n",
    "        ),\n",
    "        \"precision\": precision_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple):  # Unpack logits if it's a tuple\n",
    "        logits = logits[0]\n",
    "    return calculate_metric_with_sklearn(logits, labels)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--task\", default=None, type=str, required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_labels\", default=2, type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_path\", default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_path\", default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", default=5, type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name\", default=None, required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kmer\", type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\", default=512, type=int\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    num_labels = int(args.num_labels)\n",
    "    task_path = args.task\n",
    "    data_path = '/beegfs/ws/1/mesa972e-paper_revisions/finetuning_tasks/'\n",
    "    model_path = args.model_path\n",
    "    tokenizer_path = args.tokenizer_path\n",
    "    max_length = args.max_length\n",
    "    epochs = int(args.epochs)\n",
    "    model_name = args.model_name\n",
    "    kmer = args.kmer\n",
    "\n",
    "    output_path = data_path + task_path + \"/\"+model_name+\"/\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    train = pd.read_csv(data_path + task_path + \"/train.tsv\", sep='\\t', converters={'X': eval, 'y': eval})\n",
    "    test = pd.read_csv(data_path + task_path + \"/test.tsv\", sep='\\t', converters={'X': eval, 'y': eval})\n",
    "    val = pd.read_csv(data_path + task_path + \"/val.tsv\", sep='\\t', converters={'X': eval, 'y': eval})\n",
    "\n",
    "    \n",
    "\n",
    "    if \"NT\" in model_name:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\", num_labels=num_labels,trust_remote_code=True)\n",
    "    elif \"DNABERT2\" in model_name:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"zhihan1996/DNABERT-2-117M\", num_labels=num_labels, trust_remote_code=True)\n",
    "    else:\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
    "    \n",
    "    model.to('cuda')\n",
    "\n",
    "    if \"NT\" in model_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\",trust_remote_code=True)\n",
    "        tokenizer.eos_token = tokenizer.pad_token\n",
    "    elif \"DNABERT2\" in model_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "    elif kmer:\n",
    "        tokenizer = BertTokenizer.from_pretrained(tokenizer_path, \n",
    "                                                    do_lower_case=False, \n",
    "                                                    padding_side=\"right\",\n",
    "                                                    add_special_tokens=True\n",
    "                                                    )\n",
    "    else:\n",
    "        tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path + \"tokenizer.json\",\n",
    "                                                    do_lower_case=False,\n",
    "                                                    padding_side=\"right\"\n",
    "                                                    )\n",
    "    tokenizer.pad_token = \"[PAD]\"\n",
    "    tokenizer.sep_token = \"[SEP]\"\n",
    "    tokenizer.mask_token = \"[MASK]\"\n",
    "    tokenizer.cls_token = \"[CLS]\"\n",
    "\n",
    "    train_dataset = SupervisedDataset(train.X, train.y, tokenizer, max_length, kmer)\n",
    "    test_dataset = SupervisedDataset(test.X, test.y, tokenizer, max_length, kmer)\n",
    "    val_dataset = SupervisedDataset(val.X, val.y, tokenizer, max_length, kmer)\n",
    "   \n",
    "    \n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    optim = \"adamw_torch\"\n",
    "    per_device_train_batch_size = 16\n",
    "    per_device_eval_batch_size = 16\n",
    "    fp16 = True\n",
    "\n",
    "    evaluation_strategy = \"epoch\"\n",
    "    save_strategy = \"epoch\"\n",
    "    logging_strategy = \"epoch\"\n",
    "    warmup_steps = 50\n",
    "    weight_decay = 0.01\n",
    "    learning_rate = 1e-6\n",
    "    save_total_limit = 3\n",
    "    load_best_model_at_end = True\n",
    "    find_unused_parameters = False\n",
    "    checkpointing = False\n",
    "    dataloader_pin_memory = False\n",
    "    eval_and_save_results = True\n",
    "    save_model = False\n",
    "    seed = 42\n",
    "    overwrite_output_dir = True\n",
    "    \n",
    "\n",
    "    train_args = TrainingArguments(\n",
    "                                per_device_train_batch_size=per_device_train_batch_size,\n",
    "                                per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "                                num_train_epochs=epochs,\n",
    "                                evaluation_strategy=evaluation_strategy,\n",
    "                                save_strategy=save_strategy,\n",
    "                                logging_strategy=logging_strategy,\n",
    "                                load_best_model_at_end=load_best_model_at_end,\n",
    "                                # metric_for_best_model=\"matthews_correlation\",\n",
    "                                learning_rate=learning_rate,\n",
    "                                output_dir=output_path,\n",
    "                                warmup_steps=warmup_steps,\n",
    "                                weight_decay=weight_decay,\n",
    "                                seed=seed,\n",
    "                                save_total_limit = save_total_limit,\n",
    "                                find_unused_parameters=find_unused_parameters,\n",
    "                                checkpointing=checkpointing,\n",
    "                                dataloader_pin_memory=dataloader_pin_memory,\n",
    "                                eval_and_save_results=eval_and_save_results,\n",
    "                                save_model=save_model,\n",
    "                                optim=optim,\n",
    "                                model_max_length=max_length,\n",
    "                                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                                fp16 = fp16,\n",
    "                                overwrite_output_dir=overwrite_output_dir,\n",
    "                                eval_accumulation_steps=2\n",
    "                                )\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "                                model=model,\n",
    "                                tokenizer=tokenizer,\n",
    "                                compute_metrics=compute_metrics,\n",
    "                                train_dataset=train_dataset,\n",
    "                                eval_dataset=val_dataset,\n",
    "                                data_collator=data_collator,\n",
    "                                args = train_args\n",
    "                                   )\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_state()\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=output_path)\n",
    "\n",
    "    # get the evaluation results from trainer\n",
    "\n",
    "    results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    with open(os.path.join(output_path, \"eval_results.json\"), \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kmer TF-IDF\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, precision_score, recall_score, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, logits),\n",
    "        \"f1\": f1_score(\n",
    "            labels, logits, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"matthews_correlation\": matthews_corrcoef(\n",
    "            labels, logits\n",
    "        ),\n",
    "        \"precision\": precision_score(\n",
    "            labels, logits, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            labels, logits, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--task\", default=None, type=str, required=True # Prom300\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kmer\", type=int\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    task = args.task\n",
    "    kmer = args.kmer\n",
    "\n",
    "    print(\"kmer\", kmer)\n",
    "\n",
    "    print(\"-task\", task)\n",
    "    data_path = 'finetuning_tasks/'\n",
    "\n",
    "    output_path = data_path + task + \"/TF-IDF/\"+str(kmer)+\"mer/\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    train = pd.read_csv(data_path + task + \"/train.csv\")\n",
    "    test = pd.read_csv(data_path + task + \"/test.csv\")\n",
    "    val = pd.read_csv(data_path + task + \"/val.csv\")\n",
    "\n",
    "    sentences = [text for text in train.sequence]\n",
    "    X_train = []\n",
    "    for text in sentences:\n",
    "        X_train.append(\" \".join([text[i: i + kmer] for i in range(len(text) - kmer + 1)]))\n",
    "    \n",
    "    sentences = [text for text in test.sequence]\n",
    "    X_test = []\n",
    "    for text in sentences:\n",
    "        X_test.append(\" \".join([text[i: i + kmer] for i in range(len(text) - kmer + 1)]))\n",
    "\n",
    "    sentences = [text for text in val.sequence]\n",
    "    X_val = []\n",
    "    for text in sentences:\n",
    "        X_val.append(\" \".join([text[i: i + kmer] for i in range(len(text) - kmer + 1)]))\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_val = vectorizer.transform(X_val)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    pickle.dump(vectorizer, open(output_path + \"/TfidfVectorizer.pkl\", \"wb\"))\n",
    "\n",
    "    best_val_mcc = 0\n",
    "\n",
    "    for n_estimators in range(100, 5001, 100):\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, random_state=0, n_jobs=-1)\n",
    "\n",
    "        clf.fit(X_train, train.label)\n",
    "\n",
    "        y_val = clf.predict(X_val)\n",
    "\n",
    "        scores_val = calculate_metric_with_sklearn(val.label, y_val)\n",
    "\n",
    "        if scores_val[\"matthews_correlation\"] > best_val_mcc:\n",
    "            pickle.dump(clf, open(output_path + \"/RF_\"+str(n_estimators)+\".pkl\", \"wb\"))\n",
    "            \n",
    "            y_test = clf.predict(X_test)\n",
    "\n",
    "            scores_test = calculate_metric_with_sklearn(test.label, y_test)\n",
    "            with open(os.path.join(output_path, \"eval_results.json\"), \"w\") as f:\n",
    "                json.dump(scores_test, f)\n",
    "            \n",
    "            best_val_mcc = scores_test[\"matthews_correlation\"]\n",
    "                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 6J\n",
    "Performance for the NT tasks, tasks for which human data are available from the Nucleotide Transformer study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, TrainingArguments, BertTokenizer, PreTrainedTokenizerFast, AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.metrics import matthews_corrcoef, precision_score, recall_score, accuracy_score, f1_score\n",
    "import json\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)\n",
    "\n",
    "def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    valid_mask = labels != -100  \n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_labels = labels[valid_mask]\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(valid_labels, valid_predictions),\n",
    "        \"f1\": f1_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"matthews_correlation\": matthews_corrcoef(\n",
    "            valid_labels, valid_predictions\n",
    "        ),\n",
    "        \"precision\": precision_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            valid_labels, valid_predictions, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple): \n",
    "        logits = logits[0]\n",
    "    return calculate_metric_with_sklearn(logits, labels)\n",
    "     \n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--task\", default=None, type=str, required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_labels\", default=2, type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_path\", default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_path\", default=None\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", default=5, type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name\", default=None, required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kmer\", type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\", default=512, type=int\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    num_labels = int(args.num_labels)\n",
    "    task_path = args.task\n",
    "    model_path = args.model_path\n",
    "    tokenizer_path = args.tokenizer_path\n",
    "    max_length = args.max_length\n",
    "    epochs = int(args.epochs)\n",
    "    model_name = args.model_name\n",
    "    kmer = args.kmer\n",
    "\n",
    "    output_path = task_path + \"/\"+model_name+\"/\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "\n",
    "    if \"NT\" in model_name:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\", num_labels=num_labels,trust_remote_code=True)\n",
    "    elif \"DNABERT2\" in model_name:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"zhihan1996/DNABERT-2-117M\", num_labels=num_labels, trust_remote_code=True)\n",
    "    else:\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
    "    \n",
    "    if \"NT\" in model_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\",trust_remote_code=True)\n",
    "        tokenizer.eos_token = tokenizer.pad_token\n",
    "    elif \"DNABERT2\" in model_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "    elif kmer:\n",
    "        tokenizer = BertTokenizer.from_pretrained(tokenizer_path, \n",
    "                                                    do_lower_case=False, \n",
    "                                                    padding_side=\"right\",\n",
    "                                                    add_special_tokens=True\n",
    "                                                    )\n",
    "    else:\n",
    "        tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path + \"tokenizer.json\",\n",
    "                                                    do_lower_case=False,\n",
    "                                                    padding_side=\"right\"\n",
    "                                                    )\n",
    "    tokenizer.pad_token = \"[PAD]\"\n",
    "    tokenizer.sep_token = \"[SEP]\"\n",
    "    tokenizer.mask_token = \"[MASK]\"\n",
    "    tokenizer.cls_token = \"[CLS]\"\n",
    "\n",
    "    model.to('cuda')\n",
    "\n",
    "\n",
    "    dataset_name = task_path\n",
    "    train_dataset = load_dataset(\n",
    "            \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
    "            dataset_name,\n",
    "            split=\"train\",\n",
    "            streaming= False,\n",
    "        )\n",
    "    test_dataset = load_dataset(\n",
    "            \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
    "            dataset_name,\n",
    "            split=\"test\",\n",
    "            streaming= False,\n",
    "        )\n",
    "    \n",
    "    # Get training data\n",
    "    train_sequences = train_dataset['sequence']\n",
    "    train_labels = train_dataset['label']\n",
    "\n",
    "    # Split the dataset into a training and a validation dataset\n",
    "    train_sequences, validation_sequences, train_labels, validation_labels = train_test_split(train_sequences,\n",
    "                                                                                train_labels, test_size=0.05, random_state=42)\n",
    "\n",
    "    # Get test data\n",
    "    test_sequences = test_dataset['sequence']\n",
    "    test_labels = test_dataset['label']\n",
    "    \n",
    "\n",
    "    ds_train = Dataset.from_dict({\"data\": train_sequences,'labels':train_labels})\n",
    "    ds_validation = Dataset.from_dict({\"data\": validation_sequences,'labels':validation_labels})\n",
    "    ds_test = Dataset.from_dict({\"data\": test_sequences,'labels':test_labels})\n",
    "     \n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(examples[\"data\"], max_length=max_length, padding=\"longest\", truncation=True)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    # Creating tokenized promoter dataset\n",
    "    tokenized_datasets_train = ds_train.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"data\"],\n",
    "    )\n",
    "    tokenized_datasets_validation = ds_validation.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"data\"],\n",
    "    )\n",
    "    tokenized_datasets_test = ds_test.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"data\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    batch_size = 8\n",
    "    args = TrainingArguments(\n",
    "        remove_unused_columns=False,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps= 1,\n",
    "        per_device_eval_batch_size= 64,\n",
    "        num_train_epochs= epochs,\n",
    "        logging_steps= 100,\n",
    "        eval_steps = 100,\n",
    "        save_steps = 100,\n",
    "        load_best_model_at_end=True,  # Keep the best model according to the evaluation\n",
    "        metric_for_best_model=\"matthews_correlation\",\n",
    "        label_names=[\"labels\"],\n",
    "        # dataloader_drop_last=True,\n",
    "        # max_steps= 1000,\n",
    "        output_dir=output_path\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset= tokenized_datasets_train,\n",
    "        eval_dataset= tokenized_datasets_validation,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_state()\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=output_path)\n",
    "\n",
    "    # get the evaluation results from trainer\n",
    "\n",
    "    results = trainer.evaluate(eval_dataset=tokenized_datasets_test)\n",
    "    print(results)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    with open(os.path.join(output_path, \"eval_results.json\"), \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "     \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, precision_score, recall_score, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import argparse\n",
    "\n",
    "def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, logits),\n",
    "        \"f1\": f1_score(\n",
    "            labels, logits, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"matthews_correlation\": matthews_corrcoef(\n",
    "            labels, logits\n",
    "        ),\n",
    "        \"precision\": precision_score(\n",
    "            labels, logits, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            labels, logits, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--task\", default=None, type=str, required=True # \"enhancers\", \"enhancers_types\", \"promoter_all\", \"promoter_no_tata\", \"promoter_tata\", \"splice_sites_acceptors\",\"splice_sites_all\",\"splice_sites_donors\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kmer\", type=int\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    task = args.task\n",
    "    kmer = args.kmer\n",
    "\n",
    "    print(\"kmer\", kmer)\n",
    "\n",
    "    print(\"-task\", task)\n",
    "\n",
    "\n",
    "    dataset_name = task\n",
    "    train_dataset = load_dataset(\n",
    "        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
    "        dataset_name,\n",
    "        split=\"train\",\n",
    "        streaming= False,\n",
    "    )\n",
    "    test_dataset = load_dataset(\n",
    "        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
    "        dataset_name,\n",
    "        split=\"test\",\n",
    "        streaming= False,\n",
    "    )\n",
    "\n",
    "\n",
    "    output_path = task + \"/TF-IDF/\"+str(kmer)+\"mer/\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Get training data\n",
    "    train_sequences = train_dataset['sequence']\n",
    "    train_labels = train_dataset['label']\n",
    "\n",
    "    # Split the dataset into a training and a validation dataset\n",
    "    train_sequences, validation_sequences, train_labels, validation_labels = train_test_split(train_sequences,\n",
    "                                                                                train_labels, test_size=0.05, random_state=42)\n",
    "\n",
    "    # Get test data\n",
    "    test_sequences = test_dataset['sequence']\n",
    "    test_labels = test_dataset['label']\n",
    "\n",
    "    X_train = []\n",
    "    for text in train_sequences:\n",
    "        X_train.append(\" \".join([text[i: i + kmer] for i in range(len(text) - kmer + 1)]))\n",
    "    \n",
    "    X_test = []\n",
    "    for text in test_sequences:\n",
    "        X_test.append(\" \".join([text[i: i + kmer] for i in range(len(text) - kmer + 1)]))\n",
    "\n",
    "    X_val = []\n",
    "    for text in validation_sequences:\n",
    "        X_val.append(\" \".join([text[i: i + kmer] for i in range(len(text) - kmer + 1)]))\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_val = vectorizer.transform(X_val)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    pickle.dump(vectorizer, open(output_path + \"/TfidfVectorizer.pkl\", \"wb\"))\n",
    "\n",
    "    best_val_mcc = -1\n",
    "\n",
    "    for n_estimators in range(100, 2001, 100):\n",
    "        \n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, random_state=0, n_jobs=-1)\n",
    "\n",
    "        clf.fit(X_train, train_labels)\n",
    "\n",
    "        y_val = clf.predict(X_val)\n",
    "\n",
    "        scores_val = calculate_metric_with_sklearn(val_labels, y_val)\n",
    "\n",
    "        if scores_val[\"matthews_correlation\"] > best_val_mcc:\n",
    "            joblib.dump(clf, output_path + \"/RF_model.joblib\", compress=3) \n",
    "            \n",
    "            y_test = clf.predict(X_test)\n",
    "\n",
    "            scores_test = calculate_metric_with_sklearn(test.label, y_test)\n",
    "            with open(os.path.join(output_path, \"eval_results.json\"), \"w\") as f:\n",
    "                json.dump(scores_test, f)\n",
    "            \n",
    "            best_val_mcc = scores_val[\"matthews_correlation\"]\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
