{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alopez/anaconda3/envs/llm_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-04 18:15:11.413980: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-04 18:15:11.428422: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-04 18:15:11.432643: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-04 18:15:11.445139: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-04 18:15:12.360959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from trl import setup_chat_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"Llama 3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# base_model = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "# fine_tuned_model = \"andrealopez/Llama-3.1-70B-Instruct-Pima-Diabetes-Clasification\"\n",
    "\n",
    "base_model = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "fine_tuned_model = \"andrealopez/Llama-3.1-8B-Instruct-Pima-Diabetes-Clasification\"\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Merge adapter with base model\n",
    "# TODO: review si hace falta o no. \n",
    "if version == \"Llama 3.2\":\n",
    "    base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model_reload, fine_tuned_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Load and serialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset shape: (154, 9)\n"
     ]
    }
   ],
   "source": [
    "def serialize_data(row, version):\n",
    "    features_text = \" \".join([f\"The {col} is {str(row[col])}.\" for col in feature_columns])\n",
    "    if version == \"Llama 3.1\":\n",
    "        return f\"Health values: {features_text}.\\nOutcome: {int(row[target_column])}\".strip()\n",
    "    else:\n",
    "        return f\"Health values: {features_text}\".strip()\n",
    "\n",
    "# Función para eliminar el valor del Outcome (específica de llama 3.1)\n",
    "def delete_label_value(row): \n",
    "    return re.sub(r'Outcome: \\d.', 'Outcome:', row)\n",
    "\n",
    "# Función para crear el prompt o mensajes de few-shot\n",
    "def few_shot_prompt(df_shots, version):\n",
    "    if version == \"Llama 3.1\":\n",
    "        return \"\\n\".join([row['serialized_row'] for _, row in df_shots.iterrows()])\n",
    "    else:\n",
    "        instruction += \" Here are some examples.\\n\"\n",
    "        messages = [{\"role\": \"system\", \"content\": instruction}]\n",
    "        for _, row in df_shots.iterrows():\n",
    "            serialized_row = row['serialized_row']\n",
    "            messages.extend([\n",
    "                {\"role\": \"user\", \"content\": serialized_row},\n",
    "                {\"role\": \"assistant\", \"content\": f\"Outcome: {row[target_column]}\"}\n",
    "            ])\n",
    "        return messages\n",
    "\n",
    "# Cargar y preparar el conjunto de datos\n",
    "test_dataset = pd.read_csv('./PIMA_dataset/test_data.csv')\n",
    "print(\"Test dataset shape:\", test_dataset.shape)\n",
    "\n",
    "# Definir la columna de destino y las columnas de características\n",
    "target_column = \"Outcome\"\n",
    "feature_columns = [col for col in test_dataset.columns if col != target_column]\n",
    "\n",
    "# Serializar los datos\n",
    "test_dataset['serialized_row'] = test_dataset.apply(lambda row: serialize_data(row, version), axis=1)\n",
    "\n",
    "# Instrucción común\n",
    "instruction = \"\"\"You are a doctor specialised in classifying patients as diabetic or non-diabetic based on their health values. Instruction: Respond only with '0' for non-diabetic or '1' for diabetic. Use the following output format: 'Outcome: 0'.\"\"\"\n",
    "\n",
    "# Configuración de few-shot\n",
    "few_shot = True\n",
    "k_shots = 6\n",
    "if few_shot:\n",
    "    df_shots = test_dataset.sample(n=k_shots, random_state=42)\n",
    "    test_dataset = test_dataset.drop(df_shots.index)\n",
    "    \n",
    "    if version == \"Llama 3.1\":\n",
    "        instruction = instruction + \" Here are some examples.\\n\" + few_shot_prompt(df_shots, version) + f\"\\nPredict the {target_column} of the next patient.\\n\"\n",
    "    else:\n",
    "        base_messages = few_shot_prompt(df_shots, version)\n",
    "else:\n",
    "    instruction += f\" Predict the {target_column} of the next patient.\"\n",
    "    if version == \"Llama 3.2\":\n",
    "        base_messages = [{\"role\": \"system\", \"content\": instruction}]\n",
    "\n",
    "# Convertir a un objeto Dataset de HuggingFace\n",
    "serialized_test_data = Dataset.from_pandas(test_dataset[[\"serialized_row\", \"Outcome\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_values = []\n",
    "output_values = []\n",
    "inference_times = []\n",
    "iterations_to_fix = []\n",
    "\n",
    "if version == \"Llama 3.1\": \n",
    "    # Inference pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        max_new_tokens=10\n",
    "    )\n",
    "\n",
    "    for i, row in test_dataset.iterrows():\n",
    "        serialized_instance = delete_label_value(row.serialized_row)\n",
    "        # Create prompt\n",
    "        prompt = instruction + serialized_instance\n",
    "\n",
    "        # Clasificate sample\n",
    "        start_time = time.time()\n",
    "        result = pipe(prompt)\n",
    "        end_time = time.time()\n",
    "        inference_time = end_time - start_time\n",
    "        inference_times.append(inference_time)\n",
    "        print(inference_time)\n",
    "\n",
    "        # Answer\n",
    "        answer = result[0]['generated_text'].strip()\n",
    "\n",
    "        # Postprocessing to check that is the outcome of the tample\n",
    "        pattern = rf\"{re.escape(serialized_instance)}\\s*['\\\"]?(\\d)['\\\"]?\"\n",
    "        # Buscar el Outcome predicho\n",
    "        match = re.search(pattern, answer, re.DOTALL)\n",
    "        if match:\n",
    "            predicted_outcome = match.group(1).strip()  # Obtener todo el contenido después y eliminar espacios en blanco\n",
    "            if int(predicted_outcome) not in [0,1]: \n",
    "                print(\"Outcome not in [0,1]: \", predicted_outcome)\n",
    "                iterations_to_fix.append(i)\n",
    "            # Solo guardo los que están bien predichos TODO: cambiar esto? tener todos los resultados\n",
    "            else: \n",
    "                output_values.append(int(predicted_outcome))\n",
    "                real_values.append(row.Outcome)\n",
    "                \n",
    "        else:\n",
    "            print(\"Not sample match founded.\")\n",
    "            print(answer)\n",
    "            iterations_to_fix.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"Llama 3.2\": \n",
    "    for instance in serialized_test_data:\n",
    "        serialized_row = instance[\"serialized_row\"]\n",
    "        messages = base_messages.copy()\n",
    "\n",
    "        print(\"Creating prompt...\")\n",
    "        messages.extend([\n",
    "            {\"role\": \"user\", \"content\": serialized_row}\n",
    "        ])\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "        print(\"Inferring...\")\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10, num_return_sequences=1)\n",
    "        end_time = time.time()\n",
    "        inference_time = end_time - start_time\n",
    "        inference_times.append(inference_time)\n",
    "        print(inference_time)\n",
    "        \n",
    "        print(\"Decoding...\")\n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Postprocessing\n",
    "        pattern = r\"assistant\\s+Outcome:\\s*([01])\"\n",
    "        # Buscar todas las coincidencias\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            # Obtener el último resultado\n",
    "            predicted_outcome = matches[-1]\n",
    "            if int(predicted_outcome) not in [0,1]: \n",
    "                print(\"Outcome not in [0,1]: \", predicted_outcome)\n",
    "                print(text)\n",
    "            # Solo guardo los que están bien predichos TODO: cambiar esto? tener todos los resultados\n",
    "            else: \n",
    "                output_values.append(int(predicted_outcome))\n",
    "                real_values.append(instance[\"Outcome\"])\n",
    "        else:\n",
    "            print(\"Not sample match founded.\")\n",
    "            print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
